<!DOCTYPE html>
<HTML>
<BODY>
<STYLE>
	h1 {
		font-size:200%;
	}
	table th,tr,td {
		border-collapse: collapse; /* share common border between cells */
		padding: 4px; /* padding within cells */
		table-layout : fixed
	}
	table th {
	background-color:lightsteelblue
	}
	/* Tooltip container */
		.tooltip {
		position: relative;
		display: inline-block;
	}
	/* Tooltip text */
	.tooltip .tooltiptext {
		visibility: hidden;
		width: 600px;
		background-color: #555;
		color: #fff;
		text-align: left;
		padding: 5px 0;
		border-radius: 6px;
		/* Position the tooltip text */
		position: absolute;
		z-index: 1;
		bottom: 125%;
		left: -100%;
		margin-left: -60px;
		/* Fade in tooltip */
		opacity: 0;
		transition: opacity 1s;
	}
	/* Tooltip arrow */
	.tooltip .tooltiptext::after {
		content: ;
		position: absolute;
		top: 100%;
		left: 50%;
		margin-left: -5px;
		border-width: 5px;
		border-style: solid;
		border-color: #555 transparent transparent transparent;
	}
	/* Show the tooltip text when you mouse over the tooltip container */
	.tooltip:hover .tooltiptext {
		visibility: visible;
		opacity: 1;
	}
</STYLE>
<h1>Profile Summary</h1>
<br>
<h3>Application: host</h3>
<h3>Created: 2019-09-13 15:11:34</h3>
<h3>Devices: xilinx_u280_xdma_201910_1-0</h3>
<h3>Msec: 1568380294463</h3>
<h3>Report name: Profile Summary</h3>
<h3>Target: Hardware Emulation</h3>
<h3>Tool version: 2019.1</h3>
<br>
<h2>OpenCL API Calls</h2>

<TABLE border="1">
<TR>
<TH>API Name</TH>
<TH>Number<br>Of Calls</TH>
<TH>Total<br>Time (ms)</TH>
<TH>Minimum<br>Time (ms)</TH>
<TH>Average<br>Time (ms)</TH>
<TH>Maximum<br>Time (ms)</TH>
</TR>
<TR>
<TD>clCreateProgramWithBinary</TD>
<TD>1</TD>
<TD>12430.100</TD>
<TD>12430.100</TD>
<TD>12430.100</TD>
<TD>12430.100</TD>
</TR>
<TR>
<TD>clFinish</TD>
<TD>2</TD>
<TD>5573.580</TD>
<TD>0.404</TD>
<TD>2786.790</TD>
<TD>5573.180</TD>
</TR>
<TR>
<TD>clEnqueueTask</TD>
<TD>3</TD>
<TD>438.883</TD>
<TD>0.313</TD>
<TD>146.294</TD>
<TD>429.697</TD>
</TR>
<TR>
<TD>clReleaseProgram</TD>
<TD>1</TD>
<TD>6.994</TD>
<TD>6.994</TD>
<TD>6.994</TD>
<TD>6.994</TD>
</TR>
<TR>
<TD>clCreateKernel</TD>
<TD>3</TD>
<TD>2.305</TD>
<TD>0.736</TD>
<TD>0.768</TD>
<TD>0.830</TD>
</TR>
<TR>
<TD>clCreateBuffer</TD>
<TD>2</TD>
<TD>1.556</TD>
<TD>0.756</TD>
<TD>0.778</TD>
<TD>0.800</TD>
</TR>
<TR>
<TD>clEnqueueMigrateMemObjects</TD>
<TD>2</TD>
<TD>0.113</TD>
<TD>0.053</TD>
<TD>0.056</TD>
<TD>0.060</TD>
</TR>
<TR>
<TD>clGetDeviceIDs</TD>
<TD>5</TD>
<TD>0.101</TD>
<TD>0.001</TD>
<TD>0.020</TD>
<TD>0.091</TD>
</TR>
<TR>
<TD>clReleaseKernel</TD>
<TD>3</TD>
<TD>0.028</TD>
<TD>0.003</TD>
<TD>0.009</TD>
<TD>0.020</TD>
</TR>
<TR>
<TD>clReleaseEvent</TD>
<TD>4</TD>
<TD>0.017</TD>
<TD>0.002</TD>
<TD>0.004</TD>
<TD>0.006</TD>
</TR>
<TR>
<TD>clGetExtensionFunctionAddress</TD>
<TD>2</TD>
<TD>0.015</TD>
<TD>0.003</TD>
<TD>0.008</TD>
<TD>0.013</TD>
</TR>
<TR>
<TD>clRetainMemObject</TD>
<TD>4</TD>
<TD>0.014</TD>
<TD>0.001</TD>
<TD>0.003</TD>
<TD>0.008</TD>
</TR>
<TR>
<TD>clReleaseCommandQueue</TD>
<TD>1</TD>
<TD>0.013</TD>
<TD>0.013</TD>
<TD>0.013</TD>
<TD>0.013</TD>
</TR>
<TR>
<TD>clSetKernelArg</TD>
<TD>6</TD>
<TD>0.011</TD>
<TD>0.001</TD>
<TD>0.002</TD>
<TD>0.005</TD>
</TR>
<TR>
<TD>clReleaseMemObject</TD>
<TD>6</TD>
<TD>0.009</TD>
<TD>0.001</TD>
<TD>0.001</TD>
<TD>0.002</TD>
</TR>
<TR>
<TD>clGetExtensionFunctionAddressForPlatform</TD>
<TD>2</TD>
<TD>0.007</TD>
<TD>0.001</TD>
<TD>0.004</TD>
<TD>0.006</TD>
</TR>
<TR>
<TD>clGetPlatformInfo</TD>
<TD>6</TD>
<TD>0.007</TD>
<TD>0.001</TD>
<TD>0.001</TD>
<TD>0.002</TD>
</TR>
<TR>
<TD>clGetDeviceInfo</TD>
<TD>2</TD>
<TD>0.007</TD>
<TD>0.003</TD>
<TD>0.003</TD>
<TD>0.004</TD>
</TR>
<TR>
<TD>clCreateContext</TD>
<TD>1</TD>
<TD>0.006</TD>
<TD>0.006</TD>
<TD>0.006</TD>
<TD>0.006</TD>
</TR>
<TR>
<TD>clCreateCommandQueue</TD>
<TD>1</TD>
<TD>0.004</TD>
<TD>0.004</TD>
<TD>0.004</TD>
<TD>0.004</TD>
</TR>
<TR>
<TD>clReleaseContext</TD>
<TD>1</TD>
<TD>0.003</TD>
<TD>0.003</TD>
<TD>0.003</TD>
<TD>0.003</TD>
</TR>
<TR>
<TD>clReleaseDevice</TD>
<TD>2</TD>
<TD>0.003</TD>
<TD>0.001</TD>
<TD>0.001</TD>
<TD>0.002</TD>
</TR>
<TR>
<TD>clRetainDevice</TD>
<TD>2</TD>
<TD>0.002</TD>
<TD>0.001</TD>
<TD>0.001</TD>
<TD>0.002</TD>
</TR>
</TABLE>
<br>
<h2>Kernel Execution (includes estimated device times)</h2>

<TABLE border="1">
<TR>
<TH>Kernel</TH>
<TH>Number Of<br>Enqueues</TH>
<TH>Total<br>Time (ms)</TH>
<TH>Minimum<br>Time (ms)</TH>
<TH>Average<br>Time (ms)</TH>
<TH>Maximum<br>Time (ms)</TH>
</TR>
<TR>
<TD>adder_stage</TD>
<TD>1</TD>
<TD>0.022</TD>
<TD>0.022</TD>
<TD>0.022</TD>
<TD>0.022</TD>
</TR>
<TR>
<TD>input_stage</TD>
<TD>1</TD>
<TD>0.021</TD>
<TD>0.021</TD>
<TD>0.021</TD>
<TD>0.021</TD>
</TR>
<TR>
<TD>output_stage</TD>
<TD>1</TD>
<TD>0.020</TD>
<TD>0.020</TD>
<TD>0.020</TD>
<TD>0.020</TD>
</TR>
</TABLE>
<br>
<h2>Compute Unit Utilization (includes estimated device times)</h2>

<TABLE border="1">
<TR>
<TH>Device</TH>
<TH>Compute Unit</TH>
<TH>Kernel</TH>
<TH>Global<br>Work Size</TH>
<TH>Local<br>Work Size</TH>
<TH>Number<br>Of Calls</TH>
<TH>Dataflow<br>Execution</TH>
<TH>Max Parallel<br>Executions</TH>
<TH>Dataflow<br>Acceleration</TH>
<TH>Total<br>Time (ms)</TH>
<TH>Minimum<br>Time (ms)</TH>
<TH>Average<br>Time (ms)</TH>
<TH>Maximum<br>Time (ms)</TH>
<TH>Clock<br>Freq (MHz)</TH>
</TR>
<TR>
<TD>xilinx_u280_xdma_201910_1-0</TD>
<TD>adder_stage_1</TD>
<TD>adder_stage</TD>
<TD>1:1:1</TD>
<TD>1:1:1</TD>
<TD>1</TD>
<TD>No</TD>
<TD>1</TD>
<TD>1.000000x</TD>
<TD>0.016</TD>
<TD>0.016</TD>
<TD>0.016</TD>
<TD>0.016</TD>
<TD>300</TD>
</TR>
<TR>
<TD>xilinx_u280_xdma_201910_1-0</TD>
<TD>input_stage_1</TD>
<TD>input_stage</TD>
<TD>1:1:1</TD>
<TD>1:1:1</TD>
<TD>1</TD>
<TD>No</TD>
<TD>1</TD>
<TD>1.000000x</TD>
<TD>0.015</TD>
<TD>0.015</TD>
<TD>0.015</TD>
<TD>0.015</TD>
<TD>300</TD>
</TR>
<TR>
<TD>xilinx_u280_xdma_201910_1-0</TD>
<TD>output_stage_1</TD>
<TD>output_stage</TD>
<TD>1:1:1</TD>
<TD>1:1:1</TD>
<TD>1</TD>
<TD>No</TD>
<TD>1</TD>
<TD>1.000000x</TD>
<TD>0.014</TD>
<TD>0.014</TD>
<TD>0.014</TD>
<TD>0.014</TD>
<TD>300</TD>
</TR>
</TABLE>
<br>
<h2>Data Transfer: Host to Global Memory</h2>

<TABLE border="1">
<TR>
<TH>Context:Number<br>of Devices</TH>
<TH>Transfer Type</TH>
<TH>Number Of<br>Buffer Transfers</TH>
<TH>Transfer<br>Rate (MB/s)</TH>
<TH>Average Bandwidth<br>Utilization (%)</TH>
<TH>Average<br>Buffer Size (KB)</TH>
<TH>Total<br>Time (ms)</TH>
<TH>Average<br>Time (ms)</TH>
</TR>
<TR>
<TD>context0:1</TD>
<TD>READ</TD>
<TD>1</TD>
<TD>N/A</TD>
<TD>N/A</TD>
<TD>16.384</TD>
<TD>N/A</TD>
<TD>N/A</TD>
</TR>
<TR>
<TD>context0:1</TD>
<TD>WRITE</TD>
<TD>1</TD>
<TD>N/A</TD>
<TD>N/A</TD>
<TD>16.384</TD>
<TD>N/A</TD>
<TD>N/A</TD>
</TR>
</TABLE>
<br>
<h2>Data Transfer: Kernels to Global Memory</h2>

<TABLE border="1">
<TR>
<TH>Device</TH>
<TH>Compute Unit/<br>Port Name</TH>
<TH>Kernel Arguments</TH>
<TH>Memory Resources</TH>
<TH>Transfer Type</TH>
<TH>Number Of<br>Transfers</TH>
<TH>Transfer<br>Rate (MB/s)</TH>
<TH>Average Bandwidth<br>Utilization (%)</TH>
<TH>Average<br>Size (KB)</TH>
<TH>Average<br>Latency (ns)</TH>
</TR>
<TR>
<TD>xilinx_u280_xdma_201910_1-0</TD>
<TD>input_stage_1/m_axi_gmem</TD>
<TD>input_r</TD>
<TD>HBM[0]</TD>
<TD>READ</TD>
<TD>256</TD>
<TD>1126.050</TD>
<TD>9.775</TD>
<TD>0.064</TD>
<TD>143.646</TD>
</TR>
</TABLE>
<br>
<h2>Data Transfer: Streams</h2>

<TABLE border="1">
<TR>
<TH>Device</TH>
<TH>Master Port</TH>
<TH>Master<br>Kernel Arguments</TH>
<TH>Slave Port</TH>
<TH>Slave<br>Kernel Arguments</TH>
<TH>Number Of<br>Transfers</TH>
<TH>Transfer<br>Rate (MB/s)</TH>
<TH>Average<br>Size (KB)</TH>
<TH>Link<br>Utilization (%)</TH>
<TH>Link<br>Starve (%)</TH>
<TH>Link<br>Stall (%)</TH>
</TR>
<TR>
<TD>xilinx_u280_xdma_201910_1-0</TD>
<TD>PIPE</TD>
<TD>N/A</TD>
<TD>adder_stage_1/p0_pipe</TD>
<TD>All</TD>
<TD>1</TD>
<TD>1033.440</TD>
<TD>16.380</TD>
<TD>75.876</TD>
<TD>20.604</TD>
<TD>3.520</TD>
</TR>
<TR>
<TD>xilinx_u280_xdma_201910_1-0</TD>
<TD>PIPE</TD>
<TD>N/A</TD>
<TD>output_stage_1/p1_pipe</TD>
<TD>All</TD>
<TD>1</TD>
<TD>1158.690</TD>
<TD>16.380</TD>
<TD>75.960</TD>
<TD>19.959</TD>
<TD>4.081</TD>
</TR>
<TR>
<TD>xilinx_u280_xdma_201910_1-0</TD>
<TD>adder_stage_1/p1_pipe</TD>
<TD>All</TD>
<TD>PIPE</TD>
<TD>All</TD>
<TD>1</TD>
<TD>1033.440</TD>
<TD>16.380</TD>
<TD>75.904</TD>
<TD>20.575</TD>
<TD>3.522</TD>
</TR>
<TR>
<TD>xilinx_u280_xdma_201910_1-0</TD>
<TD>input_stage_1/p0_pipe</TD>
<TD>All</TD>
<TD>PIPE</TD>
<TD>All</TD>
<TD>1</TD>
<TD>1125.770</TD>
<TD>16.380</TD>
<TD>75.819</TD>
<TD>21.218</TD>
<TD>2.962</TD>
</TR>
</TABLE>
<br>
<h2>Top Data Transfer: Kernels to Global Memory</h2>

<TABLE border="1">
<TR>
<TH>Device</TH>
<TH>Compute Unit</TH>
<TH>Number Of<br>Transfers</TH>
<TH>Average Bytes<br>per Transfer</TH>
<TH>Transfer<br>Efficiency (%)</TH>
<TH>Total Data<br>Transfer (MB)</TH>
<TH>Total<br>Write (MB)</TH>
<TH>Total<br>Read (MB)</TH>
<TH>Total Transfer<br>Rate (MB/s)</TH>
</TR>
<TR>
<TD>xilinx_u280_xdma_201910_1-0</TD>
<TD>input_stage_1</TD>
<TD>256</TD>
<TD>64.000</TD>
<TD>1.562</TD>
<TD>0.016</TD>
<TD>0.000</TD>
<TD>0.016</TD>
<TD>1126.050</TD>
</TR>
</TABLE>
<br>
<h2>Top Kernel Execution</h2>

<TABLE border="1">
<TR>
<TH>Kernel Instance<br>Address</TH>
<TH>Kernel</TH>
<TH>Context ID</TH>
<TH>Command<br>Queue ID</TH>
<TH>Device</TH>
<TH>Start<br>Time (ms)</TH>
<TH>Duration (ms)</TH>
<TH>Global<br>Work Size</TH>
<TH>Local<br>Work Size</TH>
</TR>
<TR>
<TD>0x560746c4a750</TD>
<TD>adder_stage</TD>
<TD>0</TD>
<TD>0</TD>
<TD>xilinx_u280_xdma_201910_1-0</TD>
<TD>0.042</TD>
<TD>0.022</TD>
<TD>1:1:1</TD>
<TD>1:1:1</TD>
</TR>
<TR>
<TD>0x560746c4a940</TD>
<TD>input_stage</TD>
<TD>0</TD>
<TD>0</TD>
<TD>xilinx_u280_xdma_201910_1-0</TD>
<TD>0.043</TD>
<TD>0.021</TD>
<TD>1:1:1</TD>
<TD>1:1:1</TD>
</TR>
<TR>
<TD>0x560746c4aaf0</TD>
<TD>output_stage</TD>
<TD>0</TD>
<TD>0</TD>
<TD>xilinx_u280_xdma_201910_1-0</TD>
<TD>0.044</TD>
<TD>0.020</TD>
<TD>1:1:1</TD>
<TD>1:1:1</TD>
</TR>
</TABLE>
<br>
<h2>Top Memory Writes: Host to Global Memory</h2>

<TABLE border="1">
<TR>
<TH>Buffer Address</TH>
<TH>Context ID</TH>
<TH>Command<br>Queue ID</TH>
<TH>Start<br>Time (ms)</TH>
<TH>Duration (ms)</TH>
<TH>Buffer Size (KB)</TH>
<TH>Writing<br>Rate (MB/s)</TH>
</TR>
<TR>
<TD>0x0</TD>
<TD>0</TD>
<TD>0</TD>
<TD>12500.300</TD>
<TD>N/A</TD>
<TD>16.384</TD>
<TD>N/A</TD>
</TR>
</TABLE>
<br>
<h2>Top Memory Reads: Host to Global Memory</h2>

<TABLE border="1">
<TR>
<TH>Buffer Address</TH>
<TH>Context ID</TH>
<TH>Command<br>Queue ID</TH>
<TH>Start<br>Time (ms)</TH>
<TH>Duration (ms)</TH>
<TH>Buffer Size (KB)</TH>
<TH>Reading<br>Rate (MB/s)</TH>
</TR>
<TR>
<TD>0x4000</TD>
<TD>0</TD>
<TD>0</TD>
<TD>18512.800</TD>
<TD>N/A</TD>
<TD>16.384</TD>
<TD>N/A</TD>
</TR>
</TABLE>
<br>
<h2>Guidance (32 met, 7 warnings)</h2>

<TABLE border="1">
<TR>
<TH>Name</TH>
<TH>Threshold</TH>
<TH>Actual</TH>
<TH>Conclusion</TH>
<TH>Details</TH>
<TH>Resolution</TH>
</TR>
<TR>
<TD>Average Read Size (KB)</TD>
<TD>> 0.512</TD>
<TD>0.064</TD>
<TD>Not Met</TD>
<TD>Kernel read size of 0.064 KB on port input_stage_1/m_axi_gmem was low.</TD>
<TD>
  <a href=" " class="tooltip">Use burst transfers and packing into full memory data width.
    <span class="tooltiptext"><html>It is recommended to use burst read transfers of vector data types to increase your read transfer sizes. Burst transfers provide efficient requests, and vectors better utilize the memory data width.<br><br>For further details, in SDx navigate to &quot;Xilinx&#8594;Open SDx Example Store&#133;&quot; and install the <i>Wide Memory Read/Write</i> designs under &quot;SDAccel Examples&#8594;Getting Started Examples&#8594;Kernel to Global Memory Examples.&quot; These kernels use inferred bursts of a wide 512-bit interface.<br><br><a href="https://www.xilinx.com/cgi-bin/docs/rdoc?v=latest;d=ug1207-sdaccel-optimization-guide.pdf">SDAccel <br>Profiling and Optimization Guide</a></html></span>
  </a>
</TD>
</TR>
<TR>
<TD>Read Bandwidth (%)</TD>
<TD>> 5.000</TD>
<TD>9.775</TD>
<TD>Met</TD>
<TD>Kernel read transfers were efficient from off-chip global memory.</TD>
<TD></TD>
</TR>
<TR>
<TD>Read Amount - Minimum (MB)</TD>
<TD>> 0.250</TD>
<TD>1.000</TD>
<TD>Met</TD>
<TD>Compute units on all devices used adequate data from host.</TD>
<TD></TD>
</TR>
<TR>
<TD>Read Amount - Maximum (MB)</TD>
<TD>< 2.000</TD>
<TD>1.000</TD>
<TD>Met</TD>
<TD>No data re-use issues were found between host and compute units.</TD>
<TD></TD>
</TR>
<TR>
<TD>Port Data Width</TD>
<TD>= 512</TD>
<TD>32</TD>
<TD>Not Met</TD>
<TD>Port output_stage_1/m_axi_gmem has a data width of 32.</TD>
<TD>
  <a href=" " class="tooltip">Utilize the entire memory data width.
    <span class="tooltiptext"><html>The width of data paths between kernels and the memory controller can be configured by the SDAccel compiler as 32, 64, 128, 256, or 512 bits depending on kernel argument types. For applications that require maximum data bandwidth between the kernel and DDR memory, it is recommended that global pointers are defined explicitly as 512-bit data types.<br><br>See Chapter 4 of <a href="https://www.xilinx.com/cgi-bin/docs/rdoc?v=latest;d=ug1207-sdaccel-optimization-guide.pdf">SDAccel <br>Profiling and Optimization Guide</a></html></span>
  </a>
</TD>
</TR>
<TR>
<TD>Port Data Width</TD>
<TD>= 512</TD>
<TD>32</TD>
<TD>Not Met</TD>
<TD>Port input_stage_1/m_axi_gmem has a data width of 32.</TD>
<TD>
  <a href=" " class="tooltip">Utilize the entire memory data width.
    <span class="tooltiptext"><html>The width of data paths between kernels and the memory controller can be configured by the SDAccel compiler as 32, 64, 128, 256, or 512 bits depending on kernel argument types. For applications that require maximum data bandwidth between the kernel and DDR memory, it is recommended that global pointers are defined explicitly as 512-bit data types.<br><br>See Chapter 4 of <a href="https://www.xilinx.com/cgi-bin/docs/rdoc?v=latest;d=ug1207-sdaccel-optimization-guide.pdf">SDAccel <br>Profiling and Optimization Guide</a></html></span>
  </a>
</TD>
</TR>
<TR>
<TD>Stream Transfer Rate (MB/s)</TD>
<TD>> 10.000</TD>
<TD>1033.440</TD>
<TD>Met</TD>
<TD>Kernel stream transfers were efficient to/from host.</TD>
<TD></TD>
</TR>
<TR>
<TD>Stream Transfer Starve (%)</TD>
<TD>< 20.000</TD>
<TD>20.604</TD>
<TD>Not Met</TD>
<TD>Kernel stream starve of 20.604% on port PIPE-adder_stage_1/p0_pipe was high.</TD>
<TD>
  <a href=" " class="tooltip">Reduce starves on kernel stream data transfers.
    <span class="tooltiptext"><html>It is recommended to minimize starves on stream ports. This is typically an inefficiency on the producer write or master side of the stream connection. Make sure the master writes consistent data to keep the slave active.<br><pre>void strm_kernel(hls::stream&lt;strm&gt; &input, hls::stream&lt;strm&gt; &output) {
#pragma HLS interface axis port=input
#pragma HLS interface axis port=output
#pragma HLS interface s_axilite port=return bundle=control
  while (true) {
    strm tin = input.read();
    strm tout;
    tout.last = 0;
    for (int i=0; i < NUM_ELEMENTS; i++) {
      // process tin and output on tout
    }
    if (tin.last) {tout.last = 1;}
    output.write(tout);
    if (tout.last) {break;}
  }
}</pre></html></span>
  </a>
</TD>
</TR>
<TR>
<TD>Stream Transfer Stall (%)</TD>
<TD>< 20.000</TD>
<TD>3.520</TD>
<TD>Met</TD>
<TD>Kernel stream stalls were efficient to/from host.</TD>
<TD></TD>
</TR>
<TR>
<TD>Stream Transfer Rate (MB/s)</TD>
<TD>> 10.000</TD>
<TD>1158.690</TD>
<TD>Met</TD>
<TD>Kernel stream transfers were efficient to/from host.</TD>
<TD></TD>
</TR>
<TR>
<TD>Stream Transfer Starve (%)</TD>
<TD>< 20.000</TD>
<TD>19.959</TD>
<TD>Met</TD>
<TD>Kernel stream starves were efficient to/from host.</TD>
<TD></TD>
</TR>
<TR>
<TD>Stream Transfer Stall (%)</TD>
<TD>< 20.000</TD>
<TD>4.081</TD>
<TD>Met</TD>
<TD>Kernel stream stalls were efficient to/from host.</TD>
<TD></TD>
</TR>
<TR>
<TD>Stream Transfer Rate (MB/s)</TD>
<TD>> 10.000</TD>
<TD>1033.440</TD>
<TD>Met</TD>
<TD>Kernel stream transfers were efficient to/from host.</TD>
<TD></TD>
</TR>
<TR>
<TD>Stream Transfer Starve (%)</TD>
<TD>< 20.000</TD>
<TD>20.575</TD>
<TD>Not Met</TD>
<TD>Kernel stream starve of 20.575% on port adder_stage_1/p1_pipe-PIPE was high.</TD>
<TD>
  <a href=" " class="tooltip">Reduce starves on kernel stream data transfers.
    <span class="tooltiptext"><html>It is recommended to minimize starves on stream ports. This is typically an inefficiency on the producer write or master side of the stream connection. Make sure the master writes consistent data to keep the slave active.<br><pre>void strm_kernel(hls::stream&lt;strm&gt; &input, hls::stream&lt;strm&gt; &output) {
#pragma HLS interface axis port=input
#pragma HLS interface axis port=output
#pragma HLS interface s_axilite port=return bundle=control
  while (true) {
    strm tin = input.read();
    strm tout;
    tout.last = 0;
    for (int i=0; i < NUM_ELEMENTS; i++) {
      // process tin and output on tout
    }
    if (tin.last) {tout.last = 1;}
    output.write(tout);
    if (tout.last) {break;}
  }
}</pre></html></span>
  </a>
</TD>
</TR>
<TR>
<TD>Stream Transfer Stall (%)</TD>
<TD>< 20.000</TD>
<TD>3.522</TD>
<TD>Met</TD>
<TD>Kernel stream stalls were efficient to/from host.</TD>
<TD></TD>
</TR>
<TR>
<TD>Stream Transfer Rate (MB/s)</TD>
<TD>> 10.000</TD>
<TD>1125.770</TD>
<TD>Met</TD>
<TD>Kernel stream transfers were efficient to/from host.</TD>
<TD></TD>
</TR>
<TR>
<TD>Stream Transfer Starve (%)</TD>
<TD>< 20.000</TD>
<TD>21.218</TD>
<TD>Not Met</TD>
<TD>Kernel stream starve of 21.218% on port input_stage_1/p0_pipe-PIPE was high.</TD>
<TD>
  <a href=" " class="tooltip">Reduce starves on kernel stream data transfers.
    <span class="tooltiptext"><html>It is recommended to minimize starves on stream ports. This is typically an inefficiency on the producer write or master side of the stream connection. Make sure the master writes consistent data to keep the slave active.<br><pre>void strm_kernel(hls::stream&lt;strm&gt; &input, hls::stream&lt;strm&gt; &output) {
#pragma HLS interface axis port=input
#pragma HLS interface axis port=output
#pragma HLS interface s_axilite port=return bundle=control
  while (true) {
    strm tin = input.read();
    strm tout;
    tout.last = 0;
    for (int i=0; i < NUM_ELEMENTS; i++) {
      // process tin and output on tout
    }
    if (tin.last) {tout.last = 1;}
    output.write(tout);
    if (tout.last) {break;}
  }
}</pre></html></span>
  </a>
</TD>
</TR>
<TR>
<TD>Stream Transfer Stall (%)</TD>
<TD>< 20.000</TD>
<TD>2.962</TD>
<TD>Met</TD>
<TD>Kernel stream stalls were efficient to/from host.</TD>
<TD></TD>
</TR>
<TR>
<TD>Memory Connections</TD>
<TD>> 0</TD>
<TD>2</TD>
<TD>Met</TD>
<TD>Memory HBM[0] was used.</TD>
<TD></TD>
</TR>
<TR>
<TD>PLRAM Usage</TD>
<TD>> 0</TD>
<TD>0</TD>
<TD>Not Met</TD>
<TD>PLRAMs were used by 0 port(s).</TD>
<TD>
  <a href=" " class="tooltip">Utilize PLRAMs to maximize performance.
    <span class="tooltiptext"><html>PLRAMs can increase your system performance by providing low-latency, high-bandwidth access to your data. To take advantage of PLRAMs, assign CL memory buffers to PLRAMs in the host code and specify corresponding port mapping using the --sp xocc linker option:<pre><i/>xocc -l --sp &lt;compute_unit_name&gt;.&lt;kernel_interface_name&gt;:PLRAM[0:3]</i></pre>For more information, see examples in the Kernel To Global Memory category from the Xilinx On-boarding Example GitHub.</html></span>
  </a>
</TD>
</TR>
<TR>
<TD>High Bandwidth Memory Usage</TD>
<TD>> 0</TD>
<TD>2</TD>
<TD>Met</TD>
<TD>High bandwidth memory (HBM) was used.</TD>
<TD></TD>
</TR>
<TR>
<TD>Memory Read Bandwidth (%)</TD>
<TD>> 5.000</TD>
<TD>9.775</TD>
<TD>Met</TD>
<TD>Memory read transfers were efficient from off-chip global memory.</TD>
<TD></TD>
</TR>
<TR>
<TD>Migrate Memory API Calls</TD>
<TD>> 0</TD>
<TD>2</TD>
<TD>Met</TD>
<TD>Migrate Memory APIs (e.g., clEnqueueMigrateMemObjects) were used.</TD>
<TD></TD>
</TR>
<TR>
<TD>Average Read Size (KB)</TD>
<TD>> 4.096</TD>
<TD>16.384</TD>
<TD>Met</TD>
<TD>Host read transfers were efficient from off-chip global memory.</TD>
<TD></TD>
</TR>
<TR>
<TD>Average Write Size (KB)</TD>
<TD>> 4.096</TD>
<TD>16.384</TD>
<TD>Met</TD>
<TD>Host write transfers were efficient to off-chip global memory.</TD>
<TD></TD>
</TR>
<TR>
<TD>Compute Unit Calls - Minimum</TD>
<TD>> 0</TD>
<TD>1</TD>
<TD>Met</TD>
<TD>Compute unit adder_stage_1 on device xilinx_u280_xdma_201910_1-0 was used.</TD>
<TD></TD>
</TR>
<TR>
<TD>Compute Unit Calls - Minimum</TD>
<TD>> 0</TD>
<TD>1</TD>
<TD>Met</TD>
<TD>Compute unit input_stage_1 on device xilinx_u280_xdma_201910_1-0 was used.</TD>
<TD></TD>
</TR>
<TR>
<TD>Compute Unit Calls - Minimum</TD>
<TD>> 0</TD>
<TD>1</TD>
<TD>Met</TD>
<TD>Compute unit output_stage_1 on device xilinx_u280_xdma_201910_1-0 was used.</TD>
<TD></TD>
</TR>
<TR>
<TD>Compute Unit Utilization (%)</TD>
<TD>> 20.000</TD>
<TD>73.536</TD>
<TD>Met</TD>
<TD>Compute unit adder_stage_1 had sufficient utilization.</TD>
<TD></TD>
</TR>
<TR>
<TD>Compute Unit Utilization (%)</TD>
<TD>> 20.000</TD>
<TD>69.631</TD>
<TD>Met</TD>
<TD>Compute unit input_stage_1 had sufficient utilization.</TD>
<TD></TD>
</TR>
<TR>
<TD>Compute Unit Utilization (%)</TD>
<TD>> 20.000</TD>
<TD>70.715</TD>
<TD>Met</TD>
<TD>Compute unit output_stage_1 had sufficient utilization.</TD>
<TD></TD>
</TR>
<TR>
<TD>Compute Unit Calls - Maximum</TD>
<TD>< 16</TD>
<TD>1</TD>
<TD>Met</TD>
<TD>Kernel adder_stage was used an adequate amount.</TD>
<TD></TD>
</TR>
<TR>
<TD>Compute Unit Calls - Maximum</TD>
<TD>< 16</TD>
<TD>1</TD>
<TD>Met</TD>
<TD>Kernel input_stage was used an adequate amount.</TD>
<TD></TD>
</TR>
<TR>
<TD>Compute Unit Calls - Maximum</TD>
<TD>< 16</TD>
<TD>1</TD>
<TD>Met</TD>
<TD>Kernel output_stage was used an adequate amount.</TD>
<TD></TD>
</TR>
<TR>
<TD>Kernel Utilization (%)</TD>
<TD>= 100.000</TD>
<TD>100.000</TD>
<TD>Met</TD>
<TD>Kernel adder_stage utilized correct amount of workgroups.</TD>
<TD></TD>
</TR>
<TR>
<TD>Kernel Utilization (%)</TD>
<TD>= 100.000</TD>
<TD>100.000</TD>
<TD>Met</TD>
<TD>Kernel input_stage utilized correct amount of workgroups.</TD>
<TD></TD>
</TR>
<TR>
<TD>Kernel Utilization (%)</TD>
<TD>= 100.000</TD>
<TD>100.000</TD>
<TD>Met</TD>
<TD>Kernel output_stage utilized correct amount of workgroups.</TD>
<TD></TD>
</TR>
<TR>
<TD>Device Utilization (ms)</TD>
<TD>> 0</TD>
<TD>0.022</TD>
<TD>Met</TD>
<TD>Device xilinx_u280_xdma_201910_1-0 was used.</TD>
<TD></TD>
</TR>
<TR>
<TD>Objects Released</TD>
<TD>true</TD>
<TD>true</TD>
<TD>Met</TD>
<TD>OpenCL objects were released by the host code.</TD>
<TD></TD>
</TR>
</TABLE>
